{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from matplotlib.image import imread\n",
    "from PIL import Image\n",
    "from random import shuffle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = [1, 0]\n",
    "dog = [0, 1]\n",
    "SIZE = 50\n",
    "\n",
    "keepRate = 0.8\n",
    "LR = 0.05\n",
    "epochs = 20\n",
    "batchSize = 10\n",
    "\n",
    "TRAIN_DIR = os.path.join(os.getcwd(), 'train')\n",
    "TEST_DIR = os.path.join(os.getcwd(), 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_img(img):\n",
    "    word_label = img.split('.')[-3]\n",
    "    if word_label == 'cat': return cat\n",
    "    elif word_label == 'dog': return dog\n",
    "    \n",
    "def resize_data():\n",
    "    for img in os.listdir(TRAIN_DIR):\n",
    "        path = os.path.join(TRAIN_DIR, img)\n",
    "        img = Image.open(path)\n",
    "        img = img.resize((SIZE, SIZE), PIL.Image.ANTIALIAS)\n",
    "        img.save(path)\n",
    "        \n",
    "    for img in os.listdir(TEST_DIR):\n",
    "        path = os.path.join(TEST_DIR, img)\n",
    "        img = Image.open(path)\n",
    "        img = img.resize((SIZE, SIZE), PIL.Image.ANTIALIAS)\n",
    "        img.save(path)\n",
    "\n",
    "def prep_data():\n",
    "    train_data_imgs = []\n",
    "    train_data_lbls = []\n",
    "    \n",
    "    for img in os.listdir(TRAIN_DIR):\n",
    "        label = label_img(img)\n",
    "        path = os.path.join(TRAIN_DIR, img)\n",
    "        img = imread(path)\n",
    "        train_data_imgs.append(np.array(img))\n",
    "        train_data_lbls.append(np.array(label))\n",
    "        \n",
    "    test_data = []\n",
    "    for img in os.listdir(TEST_DIR):\n",
    "        path = os.path.join(TEST_DIR, img)\n",
    "        img = imread(path)\n",
    "        test_data.append(img)\n",
    "        \n",
    "    return train_data_imgs, train_data_lbls, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_imgs, train_data_lbls, test_data = prep_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.array(train_data_imgs).shape)\n",
    "# print(np.array(train_data_lbls).shape)\n",
    "# print(np.array(test_data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('train_data_imgs.npy', train_data_imgs)\n",
    "# np.save('train_data_lbls', train_data_lbls)\n",
    "# np.save('test_data.npy', test_data)\n",
    "\n",
    "train_data_imgs = np.load('train_data_imgs.npy')\n",
    "train_data_lbls = np.load('train_data_lbls.npy')\n",
    "test_data = np.load('test_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 50, 50, 3)\n",
      "(25000, 2)\n",
      "(12500, 50, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(train_data_imgs).shape)\n",
    "print(np.array(train_data_lbls).shape)\n",
    "print(np.array(test_data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def maxpool2d(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, SIZE, SIZE, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "weights = {\n",
    "    'Wconv1': tf.Variable(tf.random_normal([5, 5, 3, 32])),\n",
    "    'Wconv2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    'Wconv3': tf.Variable(tf.random_normal([5, 5, 64, 128])),\n",
    "    'Wconv4': tf.Variable(tf.random_normal([5, 5, 128, 64])),\n",
    "    'Wconv5': tf.Variable(tf.random_normal([5, 5, 64, 32])),\n",
    "    'Wfc': tf.Variable(tf.random_normal([2*2*32, 1024])),\n",
    "    'Wout': tf.Variable(tf.random_normal([1024, 2]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bconv1': tf.Variable(tf.zeros([32])),\n",
    "    'bconv2': tf.Variable(tf.zeros([64])),\n",
    "    'bconv3': tf.Variable(tf.zeros([128])),\n",
    "    'bconv4': tf.Variable(tf.zeros([64])),\n",
    "    'bconv5': tf.Variable(tf.zeros([32])),\n",
    "    'bfc': tf.Variable(tf.zeros([1024])),\n",
    "    'bout': tf.Variable(tf.zeros([2]))\n",
    "}\n",
    "\n",
    "convnet = tf.nn.relu(conv2d(x, weights['Wconv1']) + biases['bconv1'])\n",
    "convnet = maxpool2d(convnet)\n",
    "\n",
    "convnet = tf.nn.relu(conv2d(convnet, weights['Wconv2']) + biases['bconv2'])\n",
    "convnet = maxpool2d(convnet)\n",
    "\n",
    "convnet = tf.nn.relu(conv2d(convnet, weights['Wconv3']) + biases['bconv3'])\n",
    "convnet = maxpool2d(convnet)\n",
    "\n",
    "convnet = tf.nn.relu(conv2d(convnet, weights['Wconv4']) + biases['bconv4'])\n",
    "convnet = maxpool2d(convnet)\n",
    "\n",
    "convnet = tf.nn.relu(conv2d(convnet, weights['Wconv5']) + biases['bconv5'])\n",
    "convnet = maxpool2d(convnet)\n",
    "\n",
    "convnet = tf.reshape(convnet, [-1, 2*2*32])\n",
    "convnet = tf.nn.relu(tf.matmul(convnet, weights['Wfc']) + biases['bfc'])\n",
    "convnet = tf.nn.dropout(convnet, keepRate)\n",
    "\n",
    "convnet = tf.matmul(convnet, weights['Wout']) + biases['bout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters:  13078952\n"
     ]
    }
   ],
   "source": [
    "prediction = convnet\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=prediction))\n",
    "optimizer = tf.train.AdadeltaOptimizer(LR).minimize(loss)\n",
    "correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "print('Total Trainable Parameters: ', np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \n",
      "Loss:  25562681000.0 \n",
      "Accuracy:  0.512\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  2 \n",
      "Loss:  14415293000.0 \n",
      "Accuracy:  0.564\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  3 \n",
      "Loss:  11634263000.0 \n",
      "Accuracy:  0.512\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  4 \n",
      "Loss:  8572043300.0 \n",
      "Accuracy:  0.546\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  5 \n",
      "Loss:  7219994000.0 \n",
      "Accuracy:  0.53\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  6 \n",
      "Loss:  5755586000.0 \n",
      "Accuracy:  0.548\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  7 \n",
      "Loss:  4765586400.0 \n",
      "Accuracy:  0.586\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  8 \n",
      "Loss:  4280105700.0 \n",
      "Accuracy:  0.54\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  9 \n",
      "Loss:  3437640000.0 \n",
      "Accuracy:  0.57\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  10 \n",
      "Loss:  3104293000.0 \n",
      "Accuracy:  0.542\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  11 \n",
      "Loss:  2804925700.0 \n",
      "Accuracy:  0.548\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  12 \n",
      "Loss:  2103194600.0 \n",
      "Accuracy:  0.566\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  13 \n",
      "Loss:  1856204900.0 \n",
      "Accuracy:  0.538\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  14 \n",
      "Loss:  1290750000.0 \n",
      "Accuracy:  0.568\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  15 \n",
      "Loss:  1106596500.0 \n",
      "Accuracy:  0.566\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  16 \n",
      "Loss:  675527550.0 \n",
      "Accuracy:  0.586\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  17 \n",
      "Loss:  641782900.0 \n",
      "Accuracy:  0.522\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  18 \n",
      "Loss:  373134180.0 \n",
      "Accuracy:  0.542\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  19 \n",
      "Loss:  309853700.0 \n",
      "Accuracy:  0.526\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  20 \n",
      "Loss:  177969400.0 \n",
      "Accuracy:  0.502\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trainX = np.array([i for i in train_data[:, 0]]).reshape([-1, 50, 50, 3])\n",
    "# trainY = np.array([[i[0], i[1]] for i in train_data[:, 1]])\n",
    "# t_trainX = trainX[:500]\n",
    "# t_trainY = trainY[:500]\n",
    "\n",
    "trainX = train_data_imgs\n",
    "trainY = train_data_lbls\n",
    "t_trainX = trainX[:500]\n",
    "t_trainY = trainY[:500]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        currentBatch = 0\n",
    "        while currentBatch < len(train_data_imgs):\n",
    "            batchX = trainX[currentBatch:currentBatch+batchSize]\n",
    "            batchY = trainY[currentBatch:currentBatch+batchSize]\n",
    "            currentBatch += batchSize\n",
    "            sess.run([optimizer], feed_dict={x: batchX, y: batchY})\n",
    "            \n",
    "        l, a = sess.run([loss, accuracy], feed_dict={x: t_trainX, y: t_trainY})\n",
    "        print('Epoch: ', epoch+1, '\\nLoss: ', l, '\\nAccuracy: ', a)\n",
    "        print('\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_tf_gpu",
   "language": "python",
   "name": "_tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
