{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viraat-chandra/Workspace/Machine_Learning/Tensorflow_GPU_Venv/_tf_gpu/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from matplotlib.image import imread\n",
    "from PIL import Image\n",
    "from random import shuffle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = [1, 0]\n",
    "dog = [0, 1]\n",
    "SIZE = 50\n",
    "\n",
    "keepRate = 0.8\n",
    "LR = 0.05\n",
    "epochs = 20\n",
    "batchSize = 10\n",
    "\n",
    "TRAIN_DIR = os.path.join(os.getcwd(), 'train')\n",
    "TEST_DIR = os.path.join(os.getcwd(), 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_img(img):\n",
    "    word_label = img.split('.')[-3]\n",
    "    if word_label == 'cat': return cat\n",
    "    elif word_label == 'dog': return dog\n",
    "    \n",
    "def resize_data():\n",
    "    for img in os.listdir(TRAIN_DIR):\n",
    "        path = os.path.join(TRAIN_DIR, img)\n",
    "        img = Image.open(path)\n",
    "        img = img.resize((SIZE, SIZE), PIL.Image.ANTIALIAS)\n",
    "        img.save(path)\n",
    "        \n",
    "    for img in os.listdir(TEST_DIR):\n",
    "        path = os.path.join(TEST_DIR, img)\n",
    "        img = Image.open(path)\n",
    "        img = img.resize((SIZE, SIZE), PIL.Image.ANTIALIAS)\n",
    "        img.save(path)\n",
    "\n",
    "def prep_data():\n",
    "    train_data_imgs = []\n",
    "    train_data_lbls = []\n",
    "    \n",
    "    for img in os.listdir(TRAIN_DIR):\n",
    "        label = label_img(img)\n",
    "        path = os.path.join(TRAIN_DIR, img)\n",
    "        img = imread(path)\n",
    "        train_data_imgs.append(np.array(img))\n",
    "        train_data_lbls.append(np.array(label))\n",
    "        \n",
    "    test_data = []\n",
    "    for img in os.listdir(TEST_DIR):\n",
    "        path = os.path.join(TEST_DIR, img)\n",
    "        img = imread(path)\n",
    "        test_data.append(img)\n",
    "        \n",
    "    return train_data_imgs, train_data_lbls, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_imgs, train_data_lbls, test_data = prep_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.array(train_data_imgs).shape)\n",
    "# print(np.array(train_data_lbls).shape)\n",
    "# print(np.array(test_data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('train_data_imgs.npy', train_data_imgs)\n",
    "# np.save('train_data_lbls', train_data_lbls)\n",
    "# np.save('test_data.npy', test_data)\n",
    "\n",
    "train_data_imgs = np.load('train_data_imgs.npy')\n",
    "train_data_lbls = np.load('train_data_lbls.npy')\n",
    "test_data = np.load('test_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 50, 50, 3)\n",
      "(25000, 2)\n",
      "(12500, 50, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(train_data_imgs).shape)\n",
    "print(np.array(train_data_lbls).shape)\n",
    "print(np.array(test_data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def maxpool2d(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, SIZE, SIZE, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "red = 13 # reduced: reduced img size\n",
    "tout = 64 # temp_out: number of output filters of last convolution layer\n",
    "\n",
    "weights = {\n",
    "    'Wconv1': tf.Variable(tf.random_normal([5, 5, 3, 32])),\n",
    "    'Wconv2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "#     'Wconv3': tf.Variable(tf.random_normal([5, 5, 64, 128])),\n",
    "#     'Wconv4': tf.Variable(tf.random_normal([5, 5, 128, 64])),\n",
    "#     'Wconv5': tf.Variable(tf.random_normal([5, 5, 64, 32])),\n",
    "    'Wfc': tf.Variable(tf.random_normal([red*red*tout, 1024])),\n",
    "    'Wout': tf.Variable(tf.random_normal([1024, 2]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bconv1': tf.Variable(tf.zeros([32])),\n",
    "    'bconv2': tf.Variable(tf.zeros([64])),\n",
    "#     'bconv3': tf.Variable(tf.zeros([128])),\n",
    "#     'bconv4': tf.Variable(tf.zeros([64])),\n",
    "#     'bconv5': tf.Variable(tf.zeros([32])),\n",
    "    'bfc': tf.Variable(tf.zeros([1024])),\n",
    "    'bout': tf.Variable(tf.zeros([2]))\n",
    "}\n",
    "\n",
    "convnet = tf.nn.relu(conv2d(x, weights['Wconv1']) + biases['bconv1'])\n",
    "convnet = maxpool2d(convnet)\n",
    "\n",
    "convnet = tf.nn.relu(conv2d(convnet, weights['Wconv2']) + biases['bconv2'])\n",
    "convnet = maxpool2d(convnet)\n",
    "\n",
    "# convnet = tf.nn.relu(conv2d(convnet, weights['Wconv3']) + biases['bconv3'])\n",
    "# convnet = maxpool2d(convnet)\n",
    "\n",
    "# convnet = tf.nn.relu(conv2d(convnet, weights['Wconv4']) + biases['bconv4'])\n",
    "# convnet = maxpool2d(convnet)\n",
    "\n",
    "# convnet = tf.nn.relu(conv2d(convnet, weights['Wconv5']) + biases['bconv5'])\n",
    "# convnet = maxpool2d(convnet)\n",
    "\n",
    "convnet = tf.reshape(convnet, [-1, red*red*tout])\n",
    "convnet = tf.nn.relu(tf.matmul(convnet, weights['Wfc']) + biases['bfc'])\n",
    "convnet = tf.nn.dropout(convnet, keepRate)\n",
    "\n",
    "convnet = tf.matmul(convnet, weights['Wout']) + biases['bout']\n",
    "# convnet = tf.nn.sigmoid(tf.matmul(convnet, weights['Wout']) + biases['bout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters:  11132354\n"
     ]
    }
   ],
   "source": [
    "prediction = convnet\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=prediction))\n",
    "optimizer = tf.train.AdagradOptimizer(LR).minimize(loss)\n",
    "correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "print('Total Trainable Parameters: ', np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 \n",
      "Loss:  6070.5234 \n",
      "Accuracy:  0.538\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  2 \n",
      "Loss:  3034.4565 \n",
      "Accuracy:  0.57\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  3 \n",
      "Loss:  2393.024 \n",
      "Accuracy:  0.514\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  4 \n",
      "Loss:  875.74133 \n",
      "Accuracy:  0.562\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  5 \n",
      "Loss:  850.58673 \n",
      "Accuracy:  0.542\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  6 \n",
      "Loss:  899.8996 \n",
      "Accuracy:  0.54\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  7 \n",
      "Loss:  534.61475 \n",
      "Accuracy:  0.536\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  8 \n",
      "Loss:  477.21677 \n",
      "Accuracy:  0.528\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  9 \n",
      "Loss:  265.7188 \n",
      "Accuracy:  0.536\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  10 \n",
      "Loss:  294.0919 \n",
      "Accuracy:  0.546\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  11 \n",
      "Loss:  237.00366 \n",
      "Accuracy:  0.526\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  12 \n",
      "Loss:  381.629 \n",
      "Accuracy:  0.528\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  13 \n",
      "Loss:  805.1571 \n",
      "Accuracy:  0.56\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  14 \n",
      "Loss:  368.5914 \n",
      "Accuracy:  0.516\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  15 \n",
      "Loss:  218.85745 \n",
      "Accuracy:  0.546\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  16 \n",
      "Loss:  360.42358 \n",
      "Accuracy:  0.53\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  17 \n",
      "Loss:  374.85315 \n",
      "Accuracy:  0.544\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  18 \n",
      "Loss:  532.6025 \n",
      "Accuracy:  0.502\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  19 \n",
      "Loss:  367.7456 \n",
      "Accuracy:  0.522\n",
      "\n",
      "\n",
      "\n",
      "Epoch:  20 \n",
      "Loss:  63.024696 \n",
      "Accuracy:  0.552\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trainX = np.array([i for i in train_data[:, 0]]).reshape([-1, 50, 50, 3])\n",
    "# trainY = np.array([[i[0], i[1]] for i in train_data[:, 1]])\n",
    "# t_trainX = trainX[:500]\n",
    "# t_trainY = trainY[:500]\n",
    "\n",
    "trainX = train_data_imgs\n",
    "trainY = train_data_lbls\n",
    "t_trainX = trainX[:500]\n",
    "t_trainY = trainY[:500]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        currentBatch = 0\n",
    "        while currentBatch < len(train_data_imgs):\n",
    "            batchX = trainX[currentBatch:currentBatch+batchSize]\n",
    "            batchY = trainY[currentBatch:currentBatch+batchSize]\n",
    "            currentBatch += batchSize\n",
    "            sess.run([optimizer], feed_dict={x: batchX, y: batchY})\n",
    "            \n",
    "        l, a = sess.run([loss, accuracy], feed_dict={x: t_trainX, y: t_trainY})\n",
    "        print('Epoch: ', epoch+1, '\\nLoss: ', l, '\\nAccuracy: ', a)\n",
    "        print('\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# managed to increase accuracy just by decreasing the number of convolutional layers\n",
    "# a bit too many were there when considering the dataset size and training time\n",
    "\n",
    "# also consider checking out the sigmoid activation for output layer combined with mean log-loss loss function instead\n",
    "# of no activation and softmax cross entropy loss\n",
    "\n",
    "# further optimisations can be made by changing the model hyperparams and lr and batch_size of minibatch gradient descent\n",
    "# furthermore maybe training can be optimised using the Adadelta optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_tf_gpu",
   "language": "python",
   "name": "_tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
